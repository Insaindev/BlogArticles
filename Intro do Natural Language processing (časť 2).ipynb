{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Who is my favourite movie.', 'A Jupyter widget could not be displayed because the widget state could not be found.', 'This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook.', 'You may be able to create the widget by running the appropriate cells.']\n",
      "Počet viet: 4\n",
      "['Dr', 'Who is my favourite movie', 'A Jupyter widget could not be displayed because the widget state could not be found', 'This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook', 'You may be able to create the widget by running the appropriate cells', '']\n",
      "Počet viet: 6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# inicializácia knižnice NLTK a Tokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "\n",
    "# vzor dokumentu\n",
    "document_example = \"Dr.Who is my favourite movie. A Jupyter widget could not be displayed because the widget state could not be found. This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook. You may be able to create the widget by running the appropriate cells.\"\n",
    "\n",
    "# Initiate trainer\n",
    "text = \"\"\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "\n",
    "# Initiate tokenizer\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "def tokenizer_split(text):\n",
    "    split_sent = tokenizer.tokenize(text)\n",
    "    return split_sent\n",
    "\n",
    "def manual_split(text):\n",
    "    sentences = re.split(r\"[.!?]\", text)\n",
    "    sentences = [sent.strip(\" \") for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "# Test tokenizer\n",
    "tokenized_sentences = tokenizer_split(document_example)\n",
    "print(sentence_tok)\n",
    "print (\"Počet viet: \" + str(len(sentence_tok)))\n",
    "\n",
    "# Test manual splitter\n",
    "sents = manual_split(document_example)\n",
    "print(sents)\n",
    "print(\"Počet viet: \" + str(len(sents)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
